# 5. How would quality be identified and addressed

## Improve the existing information structure and format
In order to make sure our new information structure will promote data security and information security to enhance the ultimate quality of the information structure, the following measures are taken:

### Data Quality: How will your structure promote maintaining and measuring quality easier? 
1) For the specific data scope, I will use variables on repos such as ‘name’, ‘description’,
‘stargazers_count’, ‘language’ and users such as ‘login’(username), ‘name’(real name),
‘followers’, ‘company’.
This can help realize the consistency of the data because we can trust that these attributes are
uniformly used across different users and repositories. At the same time, some calculated
metrics, such as the "most_popular_repo", "most_frequently_used_lanaguage",
"contribution_timeline", "active_level", etc., may need to be modified due to the change in
business needs, and we need to communicate with stakeholders in a timely manner, and on the
basis of reaching a consensus, modify the data generation plan or data scheme to update their
calculation method so as to achieve consistency (Dilmegani, 2024). In this way, may information
structure can promote maintaining and measuring quality easier.
2) For the data format, I will retrieve the data from the GitHub API endpoints in JSON format,
which is the type of structure I will be using.
We can validate the JSON responses against predefined schemas or data generation plans to
help us find out if our data retrieved have deviations from expectations and update our standards
in a timely manner according to the new data standards, enabling data calibration. We can also
have a schedule for making API calls, ensuring that data is timely and calibrated (McCord et al.,
2022). This can help my information structure promote maintaining and measuring quality in an
easier manner.

### Information Security: How will your structure enable any information security issues be identified and managed down the road?
1) For the specific data scope, I will use variables such as 'private' (return true if it is a private
repository). This can help with data classification and give repositories different levels of
security protection, e.g., it will only allow access to the private repository by providing the API
key of the repository owner. We can also establish the encryption for management strategy in
this project, create encryption for private repository data, and define the encryption standards for
handling these private and sensitive data. This can help ensure that my information structure can
enable any information security issues to be identified and managed in the future.
2) For the data access of my data, I will utilize APIs to fetch data from the GitHub API, which
will be my access method. We can back up the data from the previous API call results and store
them in a database. We can also show the updated timelines of a repository and recover our
application if any information is attacked with these backed-up data. Moreover, currently, the
bearer token or API key of the GitHub user should be passed in the GET request when retrieving
their private repository data from the GitHub APIs. Building on top of this, we can also add other
encryption strategies and policies, such as adding a check on whether it is a real person making
the request and further encrypting the bearer token to enhance information security. We can also,
in the process of the user requesting information, enhance the monitoring of attacks, attempts to
decrypt, and other undesirable acts, which can help identify and manage any information security
issues that may occur in the future (Exabeam, 2024).

References:

Dilmegani, C. (2024). Data Quality Assurance: Importance & Best Practices in 2024.
AIMultiple Research. https://research.aimultiple.com/data-quality-assurance/

Exabeam. (2024). The 12 Elements of an Information Security Policy. exabeam.
https://www.exabeam.com/explainers/information-security/the-12-elements-of-an-informationsecurity-
policy/

McCord, S. E., et al. (2022). Ten Practical Questions to improve data quality, Rangelands,
Volume 44, Issue 1, 2022, Pages 17-28, ISSN 0190-0528,
https://doi.org/10.1016/j.rala.2021.07.006.
(https://www.sciencedirect.com/science/article/pii/S0190052821000699)

## Desired performance of the information system:
With these measures, the desired performance of this information system is successfully realized. Specifically, this application can:
1. Extract user and repo data from GitHub API endpoints
2. Presenting both the original data and the newly manipulated data in JSON and text format on the web page
3. Provide download options for these data
4. Download a summary of user and repo data via a GraphQL query

## Differences in quality and remediation plans
There is little differences in quality compared to the original objective of this application. However, some possible risks that may cause differences in quality and a possible remediation plan is discussed as follows.

### 1. Functional Tests with unit-testing to handle errors:
First of all, the application may experience errors during the operation, resulting in the information not being properly acquired, processed, and presented.

Therefore, in order to ensure the project is durable and robust, we will add unit testing in the application, from the user interaction with the frontend to send requests and receive responses to manipulate data to the final frontend information presentation, ensuring a 99%+ test coverage and a durable and robust implementation. We can apply Jest, a testing framework for typescript, to realize this.

To implement the test plan on an ongoing basis, first, in terms of the test content, for functional tests, I will perform unit testing on each component, such as making sure a react '<h1>' component will present a value in the MM/DD/YYYY date format. I will also perform component tests, such as whether a button can be clicked and return the result I want or whether an h4 can be re-rendered when we change its internal value. Another important test is the API test; because my project involves making API calls to the GitHub API endpoint, I'll set up tests to ensure that the correct requests are sent to the endpoint and the correct results are returned. I will incorporate the Jest testing library to realize this test plan and adopt the test-driven development (TDD) methodology, first write the test requirements for the front-end, then write the code for the front-end, and achieve a final test coverage of 99%+. 

### 2. Realize scalability, usability, and security with Amazon S3:
In addition, the application may lack scalability, usability and security if we only use it on the local host.

Therefore, we can host the application on Amazon S3 (which we have already achieved successfully), thus making it easy to access the application via a URL and also apply the security measures available on Amazon web services so that our application is secure during the API call and throughout the entire product operation. In addition, we can also use the S3 bucket in conjunction with Amazon CloudFront (a content delivery network) to help distribute the website globally and achieve scalability while reducing latency and improving load times. 

### 3. Performance testing with load and stress tests to enhance performance
In addition to this, our application may experience performance degradation in high-traffic situations, such as a decrease in response time and system crashes.

Therefore, I would like to conduct performance testing to enhance the robustness of my application. For example, I would perform load testing to see how my website performs under expected loads, such as response times and resource utilization. I can also use AWS tools and other third-party tools, such as Apache JMeter, to realize the application load testing. I would also like to conduct stress tests, such as using LoadRunner to simulate high-traffic situations to see how much traffic my website can handle at a particular time point and understand my website's limit so as to improve my website’s performance. Other performance tests I am considering include scalability testing, which focuses on scale-up and scale-out of the product, and endurance testing, which is about the long-term performance of the product. We can also apply elastic loading balancing and auto-scaling by adding the relevant AWS services to the website, enhancing the performance of the application. 

### 4. Consideration for tests, alarms, and actions to handle unexpected situations
During the implementation and operation of an application, there will always be some unexpected real-world senario that will cause the website to operate abnormally.

Therefore, we need to think about the considerations for tests, alarms, and actions to handle these situations.

In terms of the considerations for tests, I will first aim to achieve comprehensive coverage of potential failures. Moreover, the maintainability of my test code is also important; I want my tests to be easily updated as the code changes. I will also make sure that the tests are integrated into the continuous integration and delivery pipeline so that tests can automatically be run when codes are committed or pulled, therefore finding and solving problems more quickly. I will take into account real-world situations to simulate different scenarios, such as normal traffic, unusually high traffic, etc., to ensure that the tests can cover real-world situations well. At the same time, I will define key performance metrics, such as response time, error rate, etc., to ensure that the performance of our products can be well measured. 

I will set up alarms for test failures in the CI/CD pipeline and send out alarms when important performance metrics drop. For example, the alarms will be sent out when the test success rate or code coverage drops below 99%; the response times are too long, error rates exceed the limit, or the resource utilization of CPU or memory is overly high.

When we encounter problems, we have a very detailed action plan to solve them quickly. For example, when our product experiences bugs or test failures, we will immediately debug and fix the bugs, and we will do the refactoring and maintenance for the code so that the code won't break easily and the tests can be stable. We apply auto-scaling for our infrastructure so that we can handle changing loads. We will also monitor and document the functional and performance tests. The training of employees is also implemented so that problems we encountered before will not happen again.

### 5. Prevent code misuse by adding licenses
Because our application replies heavily on code, there may be legal and security concerns if others misuse our code.

Therefore, in order to prevent these disputes, we plan to add the appropriate licenses to product the legal rights of the code. We have added the MIT license to this github repository for protection and preventing misuse of our code. 

### 6. Prepare an Availability, Limitations, Ethics and Societal Impact narrative to better address user and stakeholder concerns
The Availability, Limitations, Ethics and Societal Impact narrative is prepared to help users and stakeholders better understand the application and what measures we have taken to make the application more available, ethical, and providing positive social impact, etc. 

```
My product makes API calls to get information about a GitHub user and their repositories and gets some new insights based on the data retrieved, such as what the user's most used languages are and how many stargazes and forks a repository has. Our product may entail complex issues in data availability, limitations, ethics, societal impact, and portability, which we need to discuss and provide solutions to address the potential issues.

First, regarding ethical concerns, since my project will make API calls to retrieve user data, there may be data security issues during this calling process. According to consequential ethics, data leakage can lead to serious consequences, especially if my application allows users to fill in their private API keys and this information is exposed; it may lead to the illegal use of the user's account by other people, resulting in issues such as financial losses. Therefore, we need to make efforts to protect the user's data in order to prevent serious consequences from occurring. According to non-consequential ethics, even if there is no serious consequence, taking measures to protect user data can show responsibility to users, and this process and awareness are very important. Virtue ethics emphasizes that maintaining user data security is a kind of responsible behavior and also a kind of virtue, and therefore, should be promoted (Weber & Locke, 2022). The same ethical concerns can be found in the protection of user privacy, how much user data should be revealed to the public, and whether bias is present in the process of data analysis. To address these issues, my product will try to encrypt sensitive user data to ensure that the user data we get from the GitHub API will not be easily stolen. We also perform regular security checks on the product to fix any security holes in a timely manner and reduce the likelihood of data leakage.

In terms of limitations, first, a practical limitation is that the GitHub API has a limit on the number of requests a user can make to the endpoints, and too many API calls may result in reaching the limit on the number of requests permitted, which may affect the acquisition of data. Another limitation is that these API endpoints also have a maximum amount of data that can be returned per request, and exceeding the limit may result in the request being rejected, which is not conducive to data acquisition. To address this issue, my product implements data caching to reduce the frequency of requests to the GitHub API. Furthermore, my product allows users to customize the frequency and amount of data fetched, thus ensuring a more manageable demand on the API.

Moreover, one limitation and social impact is that the data leakage of GitHub API may lead to distrust of the data shared by the open source community, blocking the spirit of innovation and the development of open source culture in society. Moreover, another limitation and social impact is that the misuse of the GitHub API may cause some organizations or developers to use illegal means to obtain the data of Github users and gain some unfair market advantages, which may damage the level playing field of the market and affect the healthy development and innovation of other organizations and developers. This limitation and social impact is quite serious. In order to solve these problems, as mentioned previously, my product encrypts the data obtained from the API and strictly enforces the restriction of access rights. At the same time, my product also ensures regular checks for security vulnerabilities to ensure that data is not leaked or stolen, helping users and the open-source community build trust in our product.

Our product also ensures data availability. To access the data supporting the results reported in my product, there are two methods: For user and user’s repo data publicly available through the API endpoint or Data publicly available in a repository, you can open a web browser and visit this URL: "https://api.github.com/users/" + username for the user info and "https://api.github.com/users/" + username + "/repos" for the users’ repo info or open postman and send a get request to these same URLs, and you should get the relevant user and repo information. For data that requires the user’s private API key to access the API endpoint, or Data cannot be shared openly but are available on request from authors, you can open postman, add the API key from the user you wish to look up as the Bearer Token, then send a get request to this url: https://api.github.com/repos/username/reponame, and you should get the relevant user and repo information. For more detailed documentation on the GitHub API endpoints, you can visit the ‘GitHub REST API documentation’ at https://docs.github.com/en/rest?apiVersion=2022-11-28.

Meanwhile, by using the GitHub API as my data source, I am able to guarantee the timeliness of the data because the API's information is updated from GitHub in real-time. Meanwhile, the data is from reliable and comprehensive GitHub datasets, and the official will follow community standards to format and present the data, so the data quality is guaranteed. However, in data privacy, there may be some difficulties because data may be leaked in the process of retrieval, so this is a limitation in the data availability.

In order to solve these problems, my product will make sure to be aware of the existing data leakage risk and ensure that the whole data transfer process is transparent to inside developers and data specialists. At the same time, we will keep in touch with GitHub officials to understand the situations of the latest data and ensure that we have the most available, timely, and high-quality data, and we will also make sure to keep an eye on the latest development of data security measures and update our security measures in a timely manner to ensure that our user's data is safe.

In terms of portability, my product follows the FAIR principle. The data and metadata of our product are easy to find by calling GitHub API endpoints through the methods mentioned above.  The access method of these data is also clear to the users, including authentication and authorization through a private API key. Interoperability is also followed in our product, where it can be integrated with other data to generate new insights, and the data would require interoperation with applications or workflows to conduct things like analysis and processing. For example, the data we retrieved from a group of GitHub users can be integrated with the general usage trend on programming languages and identify interesting analysis insights on our selected users. Finally, our data is reusable because metadata and data are well-described so that they can be replicated and/or combined in a variety of scenarios, such as using past user repo info to predict future repo behaviors (GO FAIR, 2022). The opportunities and risks of data portability mentioned in OECD (2021) are also shown in my product, where my product facilitates data flows and data sharing but, at the same time, can have digital security and privacy risks. To address these issues, our products will strictly follow FAIR principles and those criteria suggested by OECD (2021) to enhance our data portability and accessibility for users. We will also strengthen the protection of user privacy and security through the monitoring of data retrieval and usage, blocking the flow of data when necessary, and alarming users in case of abnormal behaviors in the user account.

References:

GO FAIR (2022). FAIR Principles. GO FAIR. https://www.go-fair.org/fair-principles/

OECD (2021). Mapping data portability initiatives, opportunities and challenges. OECD. 

https://www.oecd-ilibrary.org/deliver/a6edfab2-en.pdf?itemId=/content/paper/a6edfab2-en&mimeType=pdf

Weber, N., & Locke, B. (2022). Ethics of Open Data. arXiv preprint arXiv:2205.10402. https://arxiv.org/pdf/2205.10402.pdf
```
